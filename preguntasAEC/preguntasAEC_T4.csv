"texto_pregunta",tema_id,"opciones",indices_correctos,"explicacion",dificultad
"¿Cuál es el fenómeno que ha obligado a estudiar nuevas medidas para mejorar los subsistemas de memoria?",4,"[""La alta tasa de fallos de memoria caché"", ""La diferencia en el crecimiento de prestaciones entre procesadores y memoria"", ""El alto consumo energético de los sistemas de memoria""]",1,"La gran diferencia en el crecimiento de las prestaciones entre los procesadores y la memoria ha obligado a estudiar nuevas medidas para mejorar los subsistemas de memoria, como se muestra en la página 3 del documento.",dificil
"¿Cuál es la principal característica de la localidad temporal en los sistemas de memoria?",4,"[""Las posiciones de memoria recientemente referenciadas serán referenciadas de nuevo en un futuro próximo"", ""Tendencia a referenciar elementos de memoria cercanos físicamente"", ""Concentración de referencias de memoria en regiones específicas del procesador""]",0,"La localidad temporal se refiere a que las posiciones de memoria recientemente referenciadas serán referenciadas de nuevo en un futuro próximo, mientras que la localidad espacial se refiere a la tendencia a referenciar elementos cercanos a los últimos referenciados.",dificil
"En un sistema con memoria caché, ¿qué significa el término 'penalización por fallo'?",4,"[""El tiempo extra que tarda el procesador en reiniciar sus operaciones tras un error"", ""El tiempo que tarda la caché en buscar un dato en su interior"", ""El tiempo de sustitución de un bloque más el tiempo de acceso al dato""]",2,"La penalización por fallo es el tiempo que toma sustituir un bloque del nivel i más el tiempo de acceso al dato, como se indica en la página 8 del documento.",dificil
"¿Qué ocurre en una caché con política de inclusión multinivel?",4,"[""La caché L1 está contenida en L2"", ""La caché L2 no contiene datos de L1"", ""Los datos se distribuyen aleatoriamente entre L1 y L2""]",0,"En la inclusión multinivel, la caché L1 está contenida en L2 (L2 ↑↑↑ L1), lo que permite determinar la coherencia entre cachés mirando solamente en la L2.",dificil
"¿Qué métrica NO es útil para medir el rendimiento de la caché de segundo nivel?",4,"[""La tasa de fallos global"", ""La tasa de fallos local"", ""El tiempo de acceso medio a memoria""]",1,"La tasa de fallos local para la caché de segundo nivel no es una buena medida para la L2, ya que la caché de primer nivel captura la mayoría de accesos a memoria. La tasa de fallos global es más útil e indica qué fracción de los accesos a memoria que hace la CPU van a memoria principal.",dificil
"¿Qué técnica de optimización del compilador mejora la localidad temporal?",4,"[""Combinación de arrays"", ""Intercambio de iteraciones"", ""Unión de bucles""]",2,"La unión de bucles mejora la localidad temporal, ya que al fusionar el código en un único bucle, los datos que se cargan en caché pueden utilizarse para las distintas operaciones antes de desalojarse, como se explica en la página 35.",dificil
"En una caché con política de post-escritura, ¿qué significa que un bloque tenga el bit de sucio a 1?",4,"[""El bloque pertenece a un programa malicioso"", ""El contenido de la página se ha modificado"", ""La dirección física ha cambiado""]",1,"El bit de sucio que cada página tiene en el TLB significa que el contenido de la página se ha modificado, no que la dirección física haya cambiado, como se menciona en la página 22.",dificil
"¿Qué problema principal tienen las cachés virtuales en relación a los sinónimos o alias?",4,"[""Requieren mayor espacio de almacenamiento"", ""Provocan incoherencias si se modifica uno de los alias"", ""Necesitan traducción de dirección para cada acceso""]",1,"Los sinónimos o alias ocurren cuando se utilizan diferentes direcciones virtuales para la misma dirección física, lo que puede provocar incoherencias si se modifica uno de los alias y el otro mantiene su valor antiguo.",dificil
"¿Cuál es el propósito principal del buffer de escritura en un sistema de memoria?",4,"[""Incrementar la tasa de aciertos"", ""Permitir la continuación de la ejecución mientras se realiza una escritura"", ""Reducir el tamaño de los bloques transferidos""]",1,"El buffer de escritura permite que las escrituras dejen el dato en el buffer, en lugar de esperar a que se escriba en memoria, de forma que las siguientes instrucciones puedan seguir ejecutándose.",dificil
"¿Cuál es la principal ventaja de las cachés indexadas virtualmente y etiquetadas físicamente?",4,"[""Permiten solventar el problema de coherencia entre caches"", ""Permiten simultanear el acceso a la caché con la traducción de la etiqueta"", ""Eliminan la necesidad de traducción de direcciones""]",1,"Estas cachés permiten simultanear el acceso (indexación) a la caché con la traducción de la etiqueta a su dirección física. Finalmente, la comparación de etiquetas se hace con direcciones físicas.",dificil
"¿Cuál es la principal desventaja de implementar un esquema de 'Acierto bajo múltiples fallos' en cachés no bloqueantes?",4,"[""Incrementa significativamente la tasa de fallos de caché"", ""Incrementa la complejidad del controlador de caché"", ""Reduce el ancho de banda disponible para el procesador""]",1,"Implementar un esquema de 'Acierto bajo múltiples fallos' incrementa la complejidad del controlador de caché, pues puede haber varios accesos al mismo tiempo a memoria, lo que aumenta la complejidad del sistema.",dificil
"¿Qué sucede cuando ocurre un fallo de lectura en una caché con buffer de escritura?",4,"[""La lectura debe esperar hasta que se vacíe el buffer de escritura"", ""Se comprueba el buffer de escritura y si no está ahí el dato se continúa con la lectura en memoria"", ""El buffer de escritura se vacía automáticamente para dar prioridad a la lectura""]",1,"Cuando se produce un fallo por una lectura, se comprueba el buffer de escritura y si no está ahí el dato buscado y el sistema de memoria está disponible, se continúa con la lectura en memoria principal.",dificil
"¿Qué técnica de optimización de la memoria principal proporciona mejor ancho de banda con menor coste?",4,"[""Mayor anchura del bus"", ""Memoria entrelazada"", ""Bancos de memoria independientes""]",1,"La técnica de memoria entrelazada es menos costosa que aumentar la anchura del bus y proporciona resultados parecidos en términos de ancho de banda, como se indica en la página 72.",dificil
"¿Cuál es el impacto principal del acceso segmentado a la caché?",4,"[""Reduce la tasa de fallos"", ""Aumenta el ancho de banda pero no disminuye la latencia"", ""Reduce la penalización por fallo""]",1,"El acceso segmentado a las cachés aumenta el ancho de banda de memoria, pero no disminuye la latencia de un acierto en caché, como se explica en la página 58.",dificil
"¿Qué característica principal define a una caché de víctimas?",4,"[""Es un buffer totalmente asociativo que guarda los bloques reemplazados del nivel superior"", ""Es una caché que prioriza los datos más frecuentemente usados"", ""Es un tipo de caché que solo almacena instrucciones""]",0,"Una caché de víctimas consiste en añadir un pequeño buffer totalmente asociativo entre un nivel de caché y el siguiente donde se quedan los datos que se van reemplazando en el nivel superior.",dificil
"¿Cuál es la principal limitación de las cachés indexadas virtualmente y etiquetadas físicamente?",4,"[""El campo índice no puede exceder el campo offset de página"", ""Requieren doble traducción de direcciones"", ""No pueden implementarse en procesadores de 64 bits""]",0,"El problema de estas cachés es que el campo índice no puede exceder el campo offset, lo que se traduce en una limitación en el tamaño efectivo de las cachés.",dificil
"¿Qué técnica de prebúsqueda es más efectiva para accesos con patrón no secuencial?",4,"[""Prebúsqueda con stride"", ""Prebúsqueda etiquetada"", ""Prebúsqueda simple del bloque siguiente""]",0,"La prebúsqueda con stride es más efectiva para patrones de acceso no secuenciales, ya que en lugar de prebuscar el bloque i+1, busca i+x (donde x es el stride o zancada), lo que permite adaptarse a patrones regulares pero no consecutivos.",dificil
"¿Qué problema resuelve la técnica de 'blocking' en la optimización del compilador?",4,"[""Reduce la tasa de fallos al mejorar la localidad temporal en accesos a arrays multidimensionales"", ""Elimina código redundante en bucles anidados"", ""Optimiza el uso de registros del procesador""]",0,"El 'blocking' reduce la tasa de fallos al mejorar la localidad temporal, especialmente útil cuando se usan varios arrays accedidos por filas y otros por columnas, como en operaciones con matrices.",dificil
"¿Qué solución se emplea para abordar el problema de los cambios de contexto en cachés virtuales?",4,"[""Vaciar completamente la caché en cada cambio de contexto"", ""Añadir un campo PID a la etiqueta"", ""Usar solo cachés físicas en sistemas multiproceso""]",1,"Para solucionar el problema de los cambios de contexto en cachés virtuales, se añade a la etiqueta un campo PID que asigna el SO, de forma que sólo necesita vaciar la caché cuando se reutilice un PID, como se explica en la página 63.",dificil
"En el contexto de la jerarquía de memoria, ¿qué significa el principio de 'inclusión'?",4,"[""Cualquier información almacenada en un nivel debe encontrarse también en los niveles inferiores"", ""Los niveles superiores deben contener todos los datos de los niveles inferiores"", ""La información debe estar distribuida de manera uniforme en todos los niveles""]",0,"El principio de inclusión establece que cualquier información almacenada en un nivel de memoria debe encontrarse también en los niveles inferiores, como se explica en la página 7 del documento.",dificil
"¿Qué determina principalmente la 'tasa de fallos local' en un sistema con múltiples niveles de caché?",4,"[""El número de fallos en la caché dividido por el número de accesos generados por la CPU"", ""El número de fallos en la caché dividido por el número total de accesos a esa caché"", ""El número de fallos en L1 dividido por los fallos en L2""]",1,"La tasa de fallos local es el número de fallos en la caché dividido por el número total de accesos a esa caché. Para la caché de segundo nivel sería mL2.",dificil
"¿Qué efecto tiene aumentar la asociatividad en una caché de segundo nivel (L2)?",4,"[""Incrementa el tiempo de servicio pero reduce la tasa de fallos"", ""Reduce el tiempo de servicio y la tasa de fallos"", ""No tiene impacto significativo en el rendimiento""]",0,"Un mayor grado de asociatividad en la L2 incrementaría el tiempo de servicio en caso de acierto, pero lo más importante para la L2 es reducir la tasa de fallos para evitar tener que ir a memoria.",dificil
"¿Qué recurso de hardware es imprescindible para implementar la técnica de prebúsqueda etiquetada?",4,"[""Un bit de etiqueta adicional por cada bloque de caché"", ""Un buffer especial para los bloques prebuscados"", ""Un contador de stride para cada dirección""]",0,"La prebúsqueda etiquetada requiere asociar un bit de etiqueta a cada bloque, inicialmente a 0. Cuando una línea es traída por fallo de caché o referenciada, el bit se pone a 1, y cuando es traída por prebúsqueda, a 0.",dificil
"En un sistema con caché multinivel, ¿cuál es la política de coherencia más adecuada entre L1 y L2?",4,"[""L1 con escritura directa y L2 con post-escritura"", ""Ambas con post-escritura"", ""Ambas con escritura directa""]",0,"La L1 y la L2 se diseñan como un todo. Un ejemplo común es tener L1 con escritura directa (WT) y L2 con post-escritura (WB), como se menciona en la página 50.",dificil
"¿Qué característica de las DRAM ha causado mayor preocupación en el diseño de sistemas de memoria modernos?",4,"[""Su capacidad limitada frente a las necesidades crecientes"", ""Su lento incremento de velocidad comparado con los procesadores"", ""Su alto consumo de energía""]",1,"La velocidad de los procesadores ha crecido más rápidamente que la de las DRAMs, haciendo que el coste relativo de un fallo de caché se incremente con el tiempo, como se menciona en la página 46.",dificil
"¿Cuál es la principal ventaja de utilizar una caché no bloqueante?",4,"[""Elimina completamente los fallos de caché"", ""Permite que la caché siga atendiendo accesos mientras resuelve un fallo"", ""Reduce el tiempo de acceso a la memoria principal""]",1,"Las cachés no bloqueantes permiten que la caché de datos siga permitiendo accesos mientras resuelve un fallo de caché de otra instrucción, lo que puede mejorar el rendimiento del sistema.",dificil
"¿Qué problema fundamental debe resolver un buffer de escritura en una caché con post-escritura?",4,"[""La coherencia entre diferentes niveles de caché"", ""El retraso en la escritura de memoria principal"", ""Posibles lecturas de datos pendientes de escritura""]",2,"En una caché con post-escritura y buffer de escritura, habría que controlar las posibles lecturas sobre un bloque que estuviera en ese buffer intermedio para evitar leer valores desactualizados.",dificil
"¿Qué limitación impone la política de coherencia 'escritura directa' en el diseño de cachés?",4,"[""Requiere mayor ancho de banda entre caché y memoria principal"", ""Limita el tamaño máximo de la caché"", ""Impide el uso de cachés multinivel""]",0,"La política de escritura directa requiere que cada escritura se realice tanto en la caché como en memoria principal, lo que aumenta el ancho de banda necesario entre ambas.",dificil
"¿Qué técnica es más efectiva para reducir el impacto de los fallos de capacidad en la caché?",4,"[""Aumentar la asociatividad"", ""Aumentar el tamaño de la caché"", ""Reducir el tamaño del bloque""]",1,"Aumentar el tamaño de la caché de segundo nivel reduce los fallos por conflicto al distribuir los datos entre más bloques y elimina muchos de los fallos de capacidad.",dificil
"¿Qué ocurre cuando se aumenta el tamaño del bloque en una caché L2?",4,"[""Aumenta la tasa de fallos por conflictos en cachés pequeñas pero mejora en grandes"", ""Siempre reduce la tasa de fallos independientemente del tamaño"", ""Siempre aumenta la penalización por fallo""]",0,"Aumentar el tamaño del bloque de la L2 aumentaría los fallos por conflicto para las cachés de segundo nivel pequeñas, pero como las cachés L2 son bastante grandes, es posible tener tamaños de bloque para la L2 de 64, 128 o 256 bytes.",dificil
"¿Qué característica define principalmente a la técnica de 'blocking' o 'bloqueo' en optimización de cachés?",4,"[""Opera sobre submatrices o bloques en vez de filas o columnas enteras"", ""Bloquea el acceso a la caché durante operaciones críticas"", ""Fusiona múltiples bloques de caché en uno solo""]",0,"La técnica de blocking consiste en que, en vez de operar sobre columnas o filas enteras, se opere sobre submatrices o bloques, maximizando el acceso a los datos de la caché antes de sustituirlos.",dificil
"¿Cuál es la principal desventaja de implementar una caché de víctimas?",4,"[""Incrementa significativamente la latencia de acceso a la caché"", ""Requiere modificaciones en el compilador"", ""Aumenta la complejidad del hardware""]",2,"Implementar una caché de víctimas incrementa la complejidad del hardware, ya que requiere añadir un buffer totalmente asociativo entre niveles de caché y gestionar adecuadamente las búsquedas en paralelo.",dificil
"¿Qué problema resuelve específicamente la técnica de 'loop exchange' o intercambio de iteraciones?",4,"[""Accesos no secuenciales a los datos en memoria"", ""Dependencias de datos entre iteraciones"", ""Conflictos de recursos en el hardware""]",0,"El intercambio de iteraciones resuelve el problema de los accesos no secuenciales a los datos en memoria, permitiendo que se accedan los datos en el orden en que están almacenados en memoria, reduciendo los fallos de caché.",dificil
"¿Qué determina principalmente el rendimiento de la jerarquía de memoria en aplicaciones con accesos aleatorios a datos?",4,"[""El tamaño de la caché L1"", ""La latencia de la caché L2"", ""La tasa de fallos global""]",2,"En aplicaciones con accesos aleatorios, la tasa de fallos global es el factor más determinante para el rendimiento, ya que indica qué fracción de los accesos a memoria que hace la CPU terminan yendo a memoria principal.",dificil
"¿Cuál es la ventaja principal de utilizar una política de exclusión multinivel en cachés?",4,"[""Maximiza el espacio efectivo de caché disponible"", ""Simplifica el protocolo de coherencia"", ""Reduce la latencia de acceso a L2""]",0,"La exclusión multinivel (L2 ≈ L1) maximiza el espacio efectivo de caché, ya que no se permite tener en L2 una copia de un dato que esté en L1, lo que aprovecha mejor el espacio total disponible.",dificil
"¿Qué efecto tiene el segmentado del acceso a caché en la penalización por fallos de predicción de saltos?",4,"[""La reduce significativamente"", ""No tiene ningún efecto"", ""La aumenta""]",2,"El acceso segmentado a las cachés aumenta el número de etapas del cauce, provocando mayor penalización en caso de fallo de predicción de saltos y más ciclos entre la entrada de una carga y el uso del dato.",dificil
"¿Qué determina la elección entre un tamaño de bloque grande o pequeño para una caché?",4,"[""La velocidad del procesador"", ""La latencia y ancho de banda del nivel inferior"", ""El tipo de aplicaciones que se ejecutarán""]",1,"La selección del tamaño del bloque depende tanto de la latencia como del ancho de banda del nivel inferior. Una gran latencia y un gran ancho de banda aconsejan un tamaño de bloque grande, mientras que una baja latencia y un bajo ancho de banda aconsejan un tamaño de bloque pequeño.",dificil
"¿Qué tipo de fallos se reducen principalmente al aplicar la técnica de 'loop fusion' o unión de bucles?",4,"[""Fallos de capacidad"", ""Fallos por conflicto"", ""Fallos forzosos""]",0,"La unión de bucles mejora la localidad temporal y reduce principalmente los fallos de capacidad, ya que al fusionar código en un único bucle, los datos que se cargan en caché pueden utilizarse para las distintas operaciones antes de ser desalojados.",dificil
"¿Cuál es el mayor desafío al implementar cachés indexadas virtualmente y etiquetadas físicamente con tamaños grandes?",4,"[""Mantener la coherencia entre cachés"", ""El tiempo de traducción de las etiquetas"", ""Superar la limitación del tamaño del índice""]",2,"El campo índice no puede exceder el campo offset de página en estas cachés, lo que limita su tamaño efectivo. La solución es aumentar la asociatividad para mantener el tamaño del índice mientras se incrementa el tamaño total de la caché.",dificil
"¿Qué enfoque es más efectivo para mejorar el rendimiento de la memoria caché en sistemas con múltiples procesadores?",4,"[""Aumentar el tamaño de los bloques"", ""Implementar bancos de memoria independientes"", ""Incrementar la frecuencia de la memoria principal""]",1,"En sistemas multiprocesador, implementar bancos de memoria independientes es más efectivo, ya que permite múltiples accesos simultáneos a diferentes direcciones en cada banco, lo que es esencial para que las cachés no bloqueantes funcionen eficientemente.",dificil
"¿Qué técnica sería más beneficiosa para reducir la penalización por fallo en una caché L1 cuando la latencia de memoria principal es muy alta?",4,"[""Implementar una caché de víctimas"", ""Usar una caché L2 grande"", ""Aumentar la asociatividad de L1""]",1,"Usar una caché L2 grande sería lo más beneficioso, ya que la caché de segundo nivel puede ser lo suficientemente grande para capturar la mayoría de los accesos que irían a memoria principal, disminuyendo así la penalización por fallo en L1.",dificil
"¿Qué métrica refleja mejor el rendimiento de una jerarquía de memoria con múltiples niveles de caché?",4,"[""La tasa de fallos de L1"", ""El tiempo medio de acceso a memoria"", ""El ancho de banda entre L1 y L2""]",1,"El tiempo medio de acceso a memoria (Tam) es la métrica más completa, ya que incorpora el tiempo de servicio en caso de acierto, la tasa de fallos y la penalización por fallo, considerando todos los niveles de la jerarquía.",dificil
"¿Qué optimización sería más efectiva para una aplicación que realiza múltiples pasadas sobre una matriz demasiado grande para caber en caché?",4,"[""Prebúsqueda por hardware"", ""Aplicar blocking (bloques)"", ""Aumentar la asociatividad de la caché""]",1,"Aplicar blocking sería la optimización más efectiva, ya que permite trabajar con submatrices o bloques que sí caben en la caché, maximizando la reutilización de los datos antes de pasar al siguiente bloque.",dificil
"¿Qué ventaja principal ofrecen las cachés no bloqueantes con 'acierto bajo múltiples fallos' frente a las de 'acierto bajo fallo'?",4,"[""Menor complejidad del hardware"", ""Menor latencia en caso de acierto"", ""Reducción de la penalización media por fallo""]",2,"Las cachés con 'acierto bajo múltiples fallos' pueden solapar varios fallos, lo que reduce la penalización media por fallo al permitir procesar múltiples fallos de caché simultáneamente, a costa de una mayor complejidad del controlador.",dificil
"¿Cuál es la principal desventaja de incrementar el tamaño de bloque en una caché?",4,"[""Aumenta la penalización por fallo"", ""Reduce la asociatividad efectiva"", ""Incrementa el tiempo de acierto""]",0,"Incrementar el tamaño del bloque aumenta la penalización por fallo, pues cuesta más mover un bloque más grande, aunque reduce los fallos forzosos al mejorar la localidad espacial.",dificil
"¿Qué problema inherente tienen las cachés virtuales que las cachés físicas no presentan?",4,"[""Mayor latencia de acceso"", ""Problemas con sinónimos o alias"", ""Menor tamaño efectivo""]",1,"Las cachés virtuales tienen el problema inherente de los sinónimos o alias, que ocurre cuando diferentes direcciones virtuales apuntan a la misma dirección física, lo que puede causar incoherencias si se modifica un alias y el otro mantiene el valor antiguo.",dificil
"¿Cuál es la principal ventaja de la política de 'no carga en escritura' (no-write-allocate)?",4,"[""Reduce la contención en el bus de memoria"", ""Evita cargar bloques que probablemente no serán reusados"", ""Simplifica el diseño del controlador de caché""]",1,"La política de 'no carga en escritura' evita cargar en caché bloques que probablemente no serán reusados, ya que si hay una escritura aislada, modificar directamente en el nivel inferior puede ser más eficiente que contaminar la caché.",dificil
"¿Qué factor limita principalmente el rendimiento de las técnicas de prebúsqueda por hardware?",4,"[""La predicción imprecisa de accesos futuros"", ""El consumo de ancho de banda por búsquedas innecesarias"", ""La latencia de acceso a la memoria principal""]",1,"Uno de los principales problemas con la prebúsqueda es que las búsquedas anticipadas innecesarias desperdician ancho de banda, lo que puede degradar el rendimiento general del sistema de memoria.",dificil
"¿Qué técnica es más efectiva para reducir fallos de conflicto en cachés de mapeo directo sin aumentar su tamaño?",4,"[""Implementar una caché de víctimas"", ""Aumentar el tamaño del bloque"", ""Utilizar prebúsqueda por hardware""]",0,"Implementar una caché de víctimas es particularmente efectivo para reducir fallos de conflicto en cachés de mapeo directo, ya que retiene los bloques recientemente desalojados que podrían ser necesarios pronto debido a conflictos.",dificil
"¿Cuál es la principal ventaja de una caché indexada virtualmente sobre una caché físicamente indexada?",4,"[""Elimina la necesidad de traducción de direcciones para accesos que aciertan"", ""Simplifica el diseño del TLB"", ""Reduce el tamaño de las etiquetas almacenadas""]",0,"Una caché indexada virtualmente elimina la necesidad de esperar a que se traduzca la dirección virtual a física antes de acceder a la caché, lo que puede reducir la latencia de acceso para los aciertos.",dificil
"¿Qué situación podría causar el peor rendimiento en una memoria entrelazada?",4,"[""Accesos secuenciales a palabras consecutivas"", ""Accesos con un stride igual al número de bancos"", ""Accesos aleatorios a memoria""]",1,"El peor rendimiento en una memoria entrelazada ocurriría con accesos que tienen un stride (zancada) igual al número de bancos, ya que todos los accesos irían al mismo banco, eliminando el beneficio del entrelazado.",dificil
"¿Qué característica principal diferencia a los bancos de memoria independientes de la memoria entrelazada?",4,"[""El tamaño de cada banco"", ""La capacidad de realizar accesos a direcciones arbitrarias simultáneamente"", ""La velocidad de transferencia de datos""]",1,"La principal diferencia es que los bancos de memoria independientes permiten múltiples accesos independientes a direcciones diferentes en cada banco simultáneamente, mientras que la memoria entrelazada comparte un controlador de memoria.",dificil